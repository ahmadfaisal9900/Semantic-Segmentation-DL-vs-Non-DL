{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class IDD20KLLDataset(Dataset):\n",
    "    def __init__(self, image_root, mask_root, transforms=None):\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Collect all image and mask paths\n",
    "        for subdir, _, files in tqdm(os.walk(mask_root)):  \n",
    "            for file in files:\n",
    "                if file.endswith(\"_gtFine_polygons.json\"): \n",
    "                    base_name = file.replace(\"_gtFine_polygons.json\", \"\")\n",
    "                    # Construct paths\n",
    "                    mask_path = Path(subdir) / file\n",
    "                    image_path = Path(image_root) / Path(subdir).relative_to(mask_root) / f\"{base_name}_leftImg8bit.jpg\"\n",
    "\n",
    "                    if image_path.exists():\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                        self.image_paths.append(image_path)\n",
    "                    else:\n",
    "                        print(f\"Warning: Image not found for mask: {mask_path}\")\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} images\")\n",
    "        print(f\"Found {len(self.mask_paths)} masks\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        mask = self.create_class_mask(mask_path, image.size)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = transforms.Resize((224, 224))(mask)  \n",
    "            mask = np.array(mask, dtype=np.uint8)  # Ensure it is integer class indices\n",
    "            mask = torch.tensor(mask, dtype=torch.long)  \n",
    "\n",
    "        return image, mask  \n",
    "    @staticmethod\n",
    "    def create_class_mask(json_path, image_size):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        img_width, img_height = image_size\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)  # Single-channel binary mask\n",
    "\n",
    "        # Draw polygons for all labels\n",
    "        for obj in data[\"objects\"]:\n",
    "            if not obj[\"deleted\"]:\n",
    "                polygon = [(point[0], point[1]) for point in obj[\"polygon\"]]\n",
    "                # Skip polygons with fewer than 2 points\n",
    "                if len(polygon) < 2:\n",
    "                    print(f\"Warning: Skipping invalid polygon with {len(polygon)} points in {json_path}\")\n",
    "                    continue\n",
    "                img = Image.new(\"L\", (img_width, img_height), 0)  # \"L\" mode creates a single-channel grayscale image\n",
    "                ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
    "                mask += np.array(img, dtype=np.uint8)\n",
    "\n",
    "        return Image.fromarray(mask) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:00, 461.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7034 images\n",
      "Found 7034 masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape before upsampling: torch.Size([8, 512, 7, 7])\n",
      "Features shape after upsampling: torch.Size([8, 512, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 1/880 [00:07<1:48:04,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape before upsampling: torch.Size([8, 512, 7, 7])\n",
      "Features shape after upsampling: torch.Size([8, 512, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 2/880 [00:17<2:09:09,  8.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m     76\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Move data to GPU\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     80\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[16], line 46\u001b[0m, in \u001b[0;36mIDD20KLLDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths[idx]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Generate class mask from JSON\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_class_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n",
      "Cell \u001b[1;32mIn[16], line 75\u001b[0m, in \u001b[0;36mIDD20KLLDataset.create_class_mask\u001b[1;34m(json_path, image_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, (img_width, img_height), \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# \"L\" mode creates a single-channel grayscale image\u001b[39;00m\n\u001b[0;32m     74\u001b[0m         ImageDraw\u001b[38;5;241m.\u001b[39mDraw(img)\u001b[38;5;241m.\u001b[39mpolygon(polygon, outline\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m         mask \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "mask_root = \"E:\\\\Projects\\\\Finished\\\\Semantic Segmentation\\\\idd20kII\\\\gtFine\"\n",
    "image_root = \"E:\\\\Projects\\\\Finished\\\\Semantic Segmentation\\\\idd20kII\\\\leftImg8bit\"\n",
    "train_image_root = os.path.join(image_root, \"train\")\n",
    "train_mask_root = os.path.join(mask_root, \"train\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "train_dataset = IDD20KLLDataset(train_image_root, train_mask_root, transforms=transform)\n",
    "num_labels = 34  #34 Classes in Indian Driving\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "class FCNSegmentationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCNSegmentationModel, self).__init__()\n",
    "        \n",
    "        # Using a pre-trained ResNet backbone\n",
    "        self.backbone = timm.create_model(\"resnet34\", pretrained=True, features_only=True)  # Set `features_only=True`\n",
    "        \n",
    "        # The backbone returns feature maps at different layers\n",
    "        # We're going to use the last feature map before fully connected layers (after conv layers)\n",
    "        self.segmentation_head = nn.Conv2d(512, num_classes, kernel_size=1)  # 512 is the output channels of ResNet34\n",
    "\n",
    "        # Add an upsampling layer to match input size (224x224 -> 224x224)\n",
    "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone (without fully connected layers)\n",
    "        features = self.backbone(x)[-1]  # We want the last feature map output (batch_size, 512, 7, 7)\n",
    "        \n",
    "        print(f\"Features shape before upsampling: {features.shape}\")  # Should print (batch_size, 512, 7, 7)?????????\n",
    "        \n",
    "        features_up = self.upsample(features)  # Shape: (batch_size, 512, 224, 224)\n",
    "        \n",
    "        print(f\"Features shape after upsampling: {features_up.shape}\")  # Should print (batch_size, 512, 224, 224)\n",
    "\n",
    "        # Apply the segmentation head to get per-pixel predictions\n",
    "        segmentation_map = self.segmentation_head(features_up)  # Shape: (batch_size, num_classes, 224, 224)\n",
    "        return segmentation_map\n",
    "\n",
    "model = FCNSegmentationModel(num_classes=num_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = loss_fn(outputs, masks)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"resnet_segmentation_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
