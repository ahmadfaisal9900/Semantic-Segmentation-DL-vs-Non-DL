{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class IDD20KLLDataset(Dataset):\n",
    "    def __init__(self, image_root, mask_root, transforms=None):\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Collect all image and mask paths\n",
    "        for subdir, _, files in tqdm(os.walk(mask_root)):  # Iterate through mask_root\n",
    "            for file in files:\n",
    "                if file.endswith(\"_gtFine_polygons.json\"):  # JSON files\n",
    "                    # Extract the base filename (e.g., \"frame0029\")\n",
    "                    base_name = file.replace(\"_gtFine_polygons.json\", \"\")\n",
    "                    # Construct paths\n",
    "                    mask_path = Path(subdir) / file\n",
    "                    image_path = Path(image_root) / Path(subdir).relative_to(mask_root) / f\"{base_name}_leftImg8bit.jpg\"\n",
    "\n",
    "                    # Check if corresponding image exists\n",
    "                    if image_path.exists():\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                        self.image_paths.append(image_path)\n",
    "                    else:\n",
    "                        print(f\"Warning: Image not found for mask: {mask_path}\")\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} images\")\n",
    "        print(f\"Found {len(self.mask_paths)} masks\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        # Generate class mask from JSON\n",
    "        mask = self.create_class_mask(mask_path, image.size)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = transforms.Resize((512, 512))(mask)  # Resize mask to (512, 512)\n",
    "            mask = np.array(mask, dtype=np.uint8)  # Ensure it is integer class indices\n",
    "            mask = torch.tensor(mask, dtype=torch.long)  # Convert to tensor after transformations\n",
    "\n",
    "        return image, mask  # Return (image, mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_class_mask(json_path, image_size):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        img_width, img_height = image_size\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)  # Single-channel binary mask\n",
    "\n",
    "        # Draw polygons for all labels\n",
    "        for obj in data[\"objects\"]:\n",
    "            if not obj[\"deleted\"]:\n",
    "                polygon = [(point[0], point[1]) for point in obj[\"polygon\"]]\n",
    "                # Skip polygons with fewer than 2 points\n",
    "                if len(polygon) < 2:\n",
    "                    print(f\"Warning: Skipping invalid polygon with {len(polygon)} points in {json_path}\")\n",
    "                    continue\n",
    "                img = Image.new(\"L\", (img_width, img_height), 0)  # \"L\" mode creates a single-channel grayscale image\n",
    "                ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
    "                mask += np.array(img, dtype=np.uint8)\n",
    "\n",
    "        return Image.fromarray(mask)  # Return as a PIL Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:00, 503.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7034 images\n",
      "Found 7034 masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# Define Paths\n",
    "mask_root = \"E:\\\\Projects\\\\Finished\\\\Semantic Segmentation\\\\idd20kII\\\\gtFine\"\n",
    "image_root = \"E:\\\\Projects\\\\Finished\\\\Semantic Segmentation\\\\idd20kII\\\\leftImg8bit\"\n",
    "train_image_root = os.path.join(image_root, \"train\")\n",
    "train_mask_root = os.path.join(mask_root, \"train\")\n",
    "\n",
    "# Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize images and masks\n",
    "    transforms.ToTensor(),          # Convert to Tensor\n",
    "])\n",
    "\n",
    "# Create Dataset Instance\n",
    "train_dataset = IDD20KLLDataset(train_image_root, train_mask_root, transforms=transform)\n",
    "# def count_classes(dataset):\n",
    "#     unique_classes = set()\n",
    "#     for _, mask in tqdm(dataset):  # Iterate over all (image, mask) pairs\n",
    "#         unique_classes.update(torch.unique(mask).tolist())  # Add unique values in the mask\n",
    "#     return len(unique_classes), unique_classes\n",
    "\n",
    "# # Count unique classes in the dataset\n",
    "# num_classes, unique_classes = count_classes(train_dataset)\n",
    "# print(f\"Number of classes: {num_classes}\")\n",
    "# print(f\"Unique class labels: {unique_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-Net model with a ResNet34 encoder\n",
    "num_classes = 13\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # Encoder backbone\n",
    "    encoder_weights=\"imagenet\",    # Pretrained on ImageNet\n",
    "    in_channels=3,                 # Input channels (RGB)\n",
    "    classes=num_classes                     # Output channels (binary or number of classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the U-Net model with a ResNet34 encoder\n",
    "# num_classes = 13\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",        # Encoder backbone\n",
    "#     encoder_weights=\"imagenet\",    # Pretrained on ImageNet\n",
    "#     in_channels=3,                 # Input channels (RGB)\n",
    "#     classes=num_classes                     # Output channels (binary or number of classes)\n",
    "# )\n",
    "# # Loss Function: Use CrossEntropyLoss for multi-class or DiceLoss for segmentation\n",
    "# loss_fn = smp.losses.DiceLoss(mode='multiclass')  # Or use nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# # Define DataLoader\n",
    "# batch_size = 8\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "\n",
    "#     for images, masks in tqdm(train_loader):\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         masks = masks.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "\n",
    "#         loss = loss_fn(outputs, masks)\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     avg_loss = train_loss / len(train_loader)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"unet_segmentation_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:00, 669.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1055 images\n",
      "Found 1055 masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 13/132 [01:40<15:59,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping invalid polygon with 0 points in E:\\Projects\\Finished\\Semantic Segmentation\\idd20kII\\gtFine\\val\\276\\frame3532_gtFine_polygons.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 31/132 [04:21<14:11,  8.43s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 13 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, val_loader)\u001b[0m\n\u001b[0;32m     28\u001b[0m         masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;66;03m#.to(device)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 31\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 13 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"unet_segmentation_model.pth\", map_location=device))\n",
    "\n",
    "# Initialize model and load weights\n",
    "#model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader):\n",
    "            images = images#.to(device)\n",
    "            masks = masks#.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "val_image_root = os.path.join(image_root, \"val\")\n",
    "val_mask_root = os.path.join(mask_root, \"val\")\n",
    "\n",
    "# Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize images and masks\n",
    "    transforms.ToTensor(),          # Convert to Tensor\n",
    "])\n",
    "\n",
    "# Create Dataset Instance\n",
    "batch_size = 8  # Adjust batch size as needed\n",
    "val_dataset = IDD20KLLDataset(val_image_root, val_mask_root, transforms=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Evaluate model\n",
    "val_loss = evaluate_model(model, val_loader)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization Function\n",
    "def visualize_predictions(model, dataset, idx):\n",
    "    model.eval()\n",
    "    image, mask = dataset[idx]\n",
    "    image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(image)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Convert tensors to numpy\n",
    "    image = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a random prediction\n",
    "idx = 0  # Choose an index from the validation dataset\n",
    "visualize_predictions(model, val_dataset, idx=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
